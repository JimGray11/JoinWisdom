#framework name
name = "rms-pred-hourly"

#mail configurations
mail{
  api.url="http://42.62.78.140:8002/WarningEmail/"
  #multi receiver, such as ["a1@b.com","a2@b.com"]
  to=["xnzhang@jointwisdom.cn"]
}

#assembly的存放目录的绝对路径
assemblies-dir = "/hadoop/1/bi_rms_v2/models/pred_v1.3_hourly/assemblies/"
#所有assembly的配置信息
assemblies = [
  {
    name = "micro-pred-shortterm-hourly"
    index = 1
    #options ["cleaner","model"]
    type = "model"
    jar-name = "rms-ab-micro-pred-shortterm-hourly-assembly-1.3.jar"
    class-name = "cn.jw.rms.ab.pred.shortterm.hourly.Predictor"
    enable = true
  },
  {
    name = "micro-pred-fix-hourly"
    index = 2
    #options ["cleaner","model"]
    type = "model"
    jar-name = "rms-ab-micro-pred-fix-hourly-assembly-1.3.jar"
    class-name = "cn.jw.rms.ab.pred.fix.hourly.Predictor"
    enable = true
  },
  {
    name = "micro-pred-sum-hourly"
    index = 3
    #options ["cleaner","model"]
    type = "model"
    jar-name = "rms-ab-micro-pred-sum-hourly-assembly-1.3.jar"
    class-name = "cn.jw.rms.ab.pred.sum.hourly.Predictor"
    enable = true
  }
]

#assembly的参数配置
parameters = [
  {
    #(必填)assembly的唯一名称, 标识该参数列表属于哪个assembly
    name = "micro-pred-shortterm-hourly"
    #以下就为具体的参数
    #(必填)数据源所在目录
    hadoop-host= "hdfs://ns1"
    #(必填)数据源所在目录
    segbkdailysum-dir="hdfs://ns1/p/bw/bi/rms/seg_bk_daily_sum/"
    segpredconf-dir = "hdfs://ns1/p/bw/bi/rms/conf/history/predconf/"
    holidays-dir = "hdfs://ns1/p/bw/bi/rms/conf/history/specialday/"
    hotel-dir = "hdfs://ns1/p/bw/bi/rms/conf/history/htlcd"
    season-dir = "hdfs://ns1/p/bw/bi/rms/conf/history/season/"
    #(必填)处理后数据保存目录
    dist-dir="hdfs://ns1/p/bw/bi/rms/middata/history/pred_sf_pu_hourly/dt=v13"
    #(必填)清洗开始日期
    #default is "now"
    fc-date = "2016-01-01,2016-01-31"
    common-htlcd = "BW00000001"
    #default is "all"
    fc-htlcd = "all"
    field-splitter = "#"
    #(必填)pairRDD重新分区的个数, 一般为cpu核数
    pairRDD-repartition-num = 1
    #(必填)普通RDD重新分区的个数, 视情况而定, 一般为cpu核数
    RDD-repartition-num = 1
    #(必填)是否将清洗后数据保存到上面的保存目录
    save-result-to-file = true
    checkpoint-num = 200
    checkpoint-dir = "/tmp"
    #checkpoint-num = 100
    #checkpoint-dir = "hdfs://ns1/p/bw/bi/rms/spark_checkpoint"
    using-prevStepRDD=false
    test-open = true
    test-segbkdailysum-dir = "hdfs://ns1/p/bw/bi/rms/seg_bk_daily_sum/dt=20160511"
    #当test-open=true时, 作为历史数据的开始日期
    hist-start="2014-01-01"
    #当test-open=true时, 作为历史数据的结束日期
    hist-end="2015-12-31"
    #当test-open=false时, 作为计算历史数据开始日期的天数
    default-hist-days = 730
    #用提前几列的数据预测
    m = 3
    #当某个tag某预测列的历史数据不满足要求时,使用的w权重
    # the count of w should be m + 1
    singular-w = "0.4#0.3#0.3#0"
    # options ["weekday","weekendOrNot","noWeekday"]
    season-weekday-tag-type = "weekday"
    #1:Mon, 2:Tue, 3:Wed, 4:Thu, 5:Fri, 6:Sat, 7:Sun
    weekend-days = [5,6,7]

    fc-days = 2
    # If fc-type = "hourly"
    #   1.The count of PU columns should be matrix-columns.length + 1
    #   2.Last column is the sum of the values that bigger than last column index
    #   3.Need add m columns after last predict hour
    #     for instance: m = 3, fc-periods = 48
    #                   should add 3 columns 50,52,54 after 48
    # columns of fc-days = 2
    matrix-columns = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54]
    #matrix-columns = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51]
    #It's worked only when testOpen = true
    adv-fc-hours-list = [1,2,3,4,6,8,12,24,48]
    #true or false
    fault-tolerance = true
    print-w = false
  },
  {
    #(必填)assembly的唯一名称, 标识该参数列表属于哪个assembly
    name = "micro-pred-fix-hourly"
    #以下就为具体的参数
    #(必填)数据源所在目录
    hadoop-host= "hdfs://ns1"
    #(必填)数据源所在目录
    combine-result-dir = "hdfs://ns1/p/bw/bi/rms/middata/history/pred_sf_pu_hourly/dt=v13"
    inventory-dir = "hdfs://ns1/p/bw/bi/rms/rms_inventory/dt=20160511"
    segpredconf-dir = "hdfs://ns1/p/bw/bi/rms/conf/history/predconf/"
    hotel-dir = "hdfs://ns1/p/bw/bi/rms/conf/history/htlcd"
    #(必填)处理后数据保存目录
    dist-dir = "hdfs://ns1/p/bw/bi/rms/middata/history/fix_pred_sf_pu_hourly/dt=v13"
    #(必填)清洗开始日期
    #default is "now"
    fc-date = "2016-01-01,2016-01-31"
    common-htlcd = "BW00000001"
    #default is "all"
    fc-htlcd = "all"
    field-splitter = "#"
    #(必填)pairRDD重新分区的个数, 一般为cpu核数
    pairRDD-repartition-num = 4
    #(必填)普通RDD重新分区的个数, 视情况而定, 一般为cpu核数
    RDD-repartition-num = 0
    #(必填)是否将清洗后数据保存到上面的保存目录
    save-result-to-file = true
    using-prevStepRDD=false
    test-open=true
    using-htltotal = false
    fc-days = 2
    matrix-columns = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54]
  },
  {
    name = "micro-pred-sum-hourly"
    matrix_input_path = "hdfs://ns1/p/bw/bi/rms/middata/history/fix_pred_sf_pu_hourly/dt=v13"
    fc_price_input_path = "hdfs://ns1/p/bw/bi/rms/pred_avg_price"
    field_seporator = "#"
    output_path = "hdfs://ns1/p/bw/bi/rms/middata/history/pred_sf_sum_hourly/dt=v13"
    pairRDD-repartition-num = "-1"
    RDD-repartition-num = "-1"
    save-result-to-file = "true"
    hdfsHost = "local"
    fc_date = "2016-01-01,2016-01-31"
    test-open = true
    test_fc_price_input_path = "hdfs://ns1/p/bw/bi/rms/pred_avg_price/dt=20160511"
  }
]